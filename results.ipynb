{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9b5f3bef",
      "metadata": {
        "id": "9b5f3bef"
      },
      "source": [
        "# YouTube Audience Network & User Communities analysis\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook presents a comprehensive analysis of YouTube's audience network and user communities using the YouNiverse dataset. The analysis consists of two main parts:\n",
        "\n",
        "### Part 1: Video-Video Audience Network\n",
        "Constructs and analyzes an audience network where **videos are nodes** and **edges represent shared commenters**. This reveals how videos connect through their audiences and identifies hub videos that bridge different content communities.\n",
        "\n",
        "### Part 2: User Community Detection\n",
        "Analyzes user behavior patterns to identify distinct communities using two complementary approaches:\n",
        "- **Feature-based clustering**: K-means on user activity, diversity, and engagement features\n",
        "- **Graph-based clustering**: k-NN similarity graph with Louvain community detection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d648cd4",
      "metadata": {
        "id": "8d648cd4"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZdHdr9B6KHNo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdHdr9B6KHNo",
        "outputId": "4834330f-d630-4656-87a3-13d8de0d6f6b"
      },
      "outputs": [],
      "source": [
        "#### Part for Google colab #####\n",
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Add your project folder to the Python path\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# CHANGE THIS PATH to match your actual folder name in Drive\n",
        "PROJECT_PATH = '/content/drive/My Drive/ADA_project/'\n",
        "sys.path.append(PROJECT_PATH)\n",
        "\n",
        "# 3. Install missing libraries (Colab has networkx/pandas, but might need python-louvain)\n",
        "!pip install python-louvain\n",
        "\n",
        "# 4. Check if it works\n",
        "try:\n",
        "    from utils import network_builder\n",
        "    print(\"setup successful! Modules found.\")\n",
        "except ImportError:\n",
        "    print(\"Error: Could not find your modules. Check the PROJECT_PATH.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y7MiIPbNO0M2",
      "metadata": {
        "id": "Y7MiIPbNO0M2"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports\n",
        "import ast\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from collections import Counter, defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "# Third-Party Imports (Data Science & Visualization)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Third-Party Imports (Network/Graph Analysis)\n",
        "import community.community_louvain as community_louvain\n",
        "import networkx as nx\n",
        "\n",
        "# Local Application Imports\n",
        "from data import data_loader\n",
        "from utils.network_builder import build_video_projection_sparse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tBeAj0g-O3Rj",
      "metadata": {
        "id": "tBeAj0g-O3Rj"
      },
      "source": [
        "# Part 1: Safe pipeline\n",
        "Implementation of initial idea of Video-Network\n",
        "## 1.1 Load and filter\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d675d34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d675d34",
        "outputId": "a74342e5-1a9b-4c63-db51-cb3fa5d71453"
      },
      "outputs": [],
      "source": [
        "FILTERED_PATH = PROJECT_PATH + 'data/df_filtered_final.parquet'\n",
        "\n",
        "if os.path.exists(FILTERED_PATH):\n",
        "    print(\"Checkpoint 1: Loading pre-filtered data from disk...\")\n",
        "    df = pd.read_parquet(FILTERED_PATH)\n",
        "else:\n",
        "    print(\"Step 1: Loading and Filtering raw data (this takes time)...\")\n",
        "\n",
        "    # --- A. LOAD RAW DATA ---\n",
        "    # Use your data_loader to load the compressed file\n",
        "    # Adjust N_ROWS if you want the full dataset (set to None)\n",
        "    df = data_loader.load_comments_gz(\n",
        "        data_path=PROJECT_PATH + 'data/',\n",
        "        comments_file='youtube_comments.tsv.gz',\n",
        "        n_chunks=10,       # Adjust based on your RAM\n",
        "        chunksize=500_000\n",
        "    )\n",
        "\n",
        "    # --- B. RENAME COLUMNS (Fixing common issues) ---\n",
        "    if 'author' in df.columns:\n",
        "        df.rename(columns={'author': 'author_id'}, inplace=True)\n",
        "\n",
        "    print(f\"   Loaded shape: {df.shape}\")\n",
        "\n",
        "    # --- C. FILTERING (Iterative K-Core) ---\n",
        "    # Keep only users active in >5 videos and videos with >10 users\n",
        "    MIN_VIDEOS_PER_USER = 5\n",
        "    MIN_USERS_PER_VIDEO = 10\n",
        "\n",
        "    print(\"   Filtering data (removing small users/videos)...\")\n",
        "    initial_len = len(df)\n",
        "\n",
        "    # Simple iterative filter\n",
        "    for i in range(3): # Run 3 passes to stabilize\n",
        "        # Filter Users\n",
        "        user_counts = df['author_id'].value_counts()\n",
        "        valid_users = user_counts[user_counts >= MIN_VIDEOS_PER_USER].index\n",
        "        df = df[df['author_id'].isin(valid_users)]\n",
        "\n",
        "        # Filter Videos\n",
        "        video_counts = df['video_id'].value_counts()\n",
        "        valid_videos = video_counts[video_counts >= MIN_USERS_PER_VIDEO].index\n",
        "        df = df[df['video_id'].isin(valid_videos)]\n",
        "\n",
        "    print(f\"   Filtered down to {len(df):,} rows (was {initial_len:,})\")\n",
        "\n",
        "    # --- D. SAVE CHECKPOINT ---\n",
        "    df.to_parquet(FILTERED_PATH, index=False)\n",
        "    print(f\"Saved filtered data to {FILTERED_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SDI1KdYCRB1n",
      "metadata": {
        "id": "SDI1KdYCRB1n"
      },
      "source": [
        "# 1.2 Enrich with metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ho9husGRJCs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ho9husGRJCs",
        "outputId": "f5e9a8b0-508a-499d-90b8-619c67bfa833"
      },
      "outputs": [],
      "source": [
        "# We need Channel & Category attached to the comments for your new analysis.\n",
        "ENRICHED_PATH = PROJECT_PATH + 'data/df_enriched.parquet'\n",
        "\n",
        "if os.path.exists(ENRICHED_PATH) and 'channel' in pd.read_parquet(ENRICHED_PATH).columns:\n",
        "    print(\"Checkpoint 2: Loading enriched data (with channels)...\")\n",
        "    df = pd.read_parquet(ENRICHED_PATH)\n",
        "else:\n",
        "    # 1. Clean Video IDs in Main DF\n",
        "    df['video_id'] = df['video_id'].astype(str).str.strip()\n",
        "    unique_vids = df['video_id'].unique()\n",
        "\n",
        "    # 2. Fetch Metadata\n",
        "    print(f\"   Fetching metadata for {len(unique_vids)} videos...\")\n",
        "    meta_map = data_loader.load_metadata_for_videos_gz(\n",
        "        metadata_path=PROJECT_PATH + 'data/yt_metadata_en.jsonl.gz',\n",
        "        video_ids=unique_vids,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # 3. THE FIXED CATEGORY CLEANER\n",
        "    def clean_cat(c):\n",
        "        # Case 1: It's None or Empty\n",
        "        if not c:\n",
        "            return \"Unknown\"\n",
        "\n",
        "        # Case 2: It's a list -> ['Music']\n",
        "        if isinstance(c, list):\n",
        "            return c[0] if len(c) > 0 else \"Unknown\"\n",
        "\n",
        "        # Case 3: It's a string -> \"Music\" or \"['Music']\"\n",
        "        if isinstance(c, str):\n",
        "            c = c.strip()\n",
        "            if c.startswith('['):\n",
        "                try:\n",
        "                    # Try parsing \"['Music']\"\n",
        "                    parsed = ast.literal_eval(c)\n",
        "                    return parsed[0] if parsed else \"Unknown\"\n",
        "                except:\n",
        "                    return \"Unknown\"\n",
        "            # It is just \"Music\" (THIS WAS THE BUG!)\n",
        "            return c\n",
        "\n",
        "        return \"Unknown\"\n",
        "\n",
        "    # 4. Merge Logic\n",
        "    meta_data = []\n",
        "    for vid, info in meta_map.items():\n",
        "        meta_data.append({\n",
        "            'video_id': str(vid).strip(),\n",
        "            # We use the ID because the Name is missing in your file\n",
        "            'channel': str(info.get('channel', 'Unknown')).strip(),\n",
        "            'category': clean_cat(info.get('categories')),\n",
        "            'title': info.get('title', 'Unknown')\n",
        "        })\n",
        "\n",
        "    df_meta = pd.DataFrame(meta_data)\n",
        "\n",
        "    # Drop old columns to avoid duplicates\n",
        "    for col in ['channel', 'category', 'title']:\n",
        "        if col in df.columns:\n",
        "            df.drop(columns=[col], inplace=True)\n",
        "\n",
        "    # Merge\n",
        "    df = df.merge(df_meta, on='video_id', how='left')\n",
        "\n",
        "    # Fill missing\n",
        "    df.fillna({'channel': 'Unknown', 'category': 'Unknown'}, inplace=True)\n",
        "\n",
        "    # Save\n",
        "    df.to_parquet(ENRICHED_PATH, index=False)\n",
        "    print(f\"Fixed! Saved enriched data to {ENRICHED_PATH}\")\n",
        "    print(\"   Sample Category:\", df['category'].iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PdkISGcpHXMC",
      "metadata": {
        "id": "PdkISGcpHXMC"
      },
      "source": [
        "### Debug part 1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UBfZnwmTHZ9V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBfZnwmTHZ9V",
        "outputId": "e1914a3e-2e8f-4e18-8c7d-6a91fef3139b"
      },
      "outputs": [],
      "source": [
        "print(\"Reloading saved states...\")\n",
        "\n",
        "# 2. Reload the DataFrame (Try Enriched first, then Filtered)\n",
        "enriched_path = PROJECT_PATH + 'data/df_enriched.parquet'\n",
        "filtered_path = PROJECT_PATH + 'data/df_filtered_final.parquet'\n",
        "\n",
        "if os.path.exists(enriched_path):\n",
        "    print(f\"Loading Enriched Data from: {enriched_path}\")\n",
        "    df = pd.read_parquet(enriched_path)\n",
        "elif os.path.exists(filtered_path):\n",
        "    print(f\"Enriched data not found. Loading Filtered data: {filtered_path}\")\n",
        "    df = pd.read_parquet(filtered_path)\n",
        "else:\n",
        "    print(\"No saved dataframe found. You might need to run Part 1 (Load & Filter) first.\")\n",
        "\n",
        "# 3. Reload the Channel Graph (if it exists)\n",
        "channel_graph_path = PROJECT_PATH + \"data/channel_category_network.gexf\"\n",
        "\n",
        "if os.path.exists(channel_graph_path):\n",
        "    print(f\"Loading Channel Graph from: {channel_graph_path}\")\n",
        "    G_channels = nx.read_gexf(channel_graph_path)\n",
        "else:\n",
        "    print(\"Channel Graph not found on disk.\")\n",
        "\n",
        "# --- NOW RUN YOUR DEBUGGING CODE ---\n",
        "print(\"\\n DEBUG OUTPUT:\")\n",
        "if 'df' in locals():\n",
        "    print(\"1. Sample Video IDs in your DataFrame:\")\n",
        "    print(df['video_id'].head().tolist())\n",
        "    print(f\"   Type: {df['video_id'].dtype}\")\n",
        "\n",
        "    print(\"\\n2. What does the 'Enriched' DataFrame look like?\")\n",
        "    # Check if columns exist before printing to avoid errors\n",
        "    cols_to_check = [c for c in ['video_id', 'channel', 'category'] if c in df.columns]\n",
        "    print(df[cols_to_check].head(10))\n",
        "\n",
        "if 'G_channels' in locals():\n",
        "    print(\"\\n3. What are the Graph Nodes named?\")\n",
        "    print(list(G_channels.nodes())[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "auPBm6IaR5lf",
      "metadata": {
        "id": "auPBm6IaR5lf"
      },
      "source": [
        "# 1.3 Build video graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AW1J6DUkR_mB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "AW1J6DUkR_mB",
        "outputId": "5f5c705c-a2c7-443c-846b-df4e01ff3029"
      },
      "outputs": [],
      "source": [
        "GRAPH_PATH = PROJECT_PATH + 'data/video_network.gexf'\n",
        "\n",
        "\n",
        "if os.path.exists(GRAPH_PATH):\n",
        "    print(\"Checkpoint 3: Loading existing graph...\")\n",
        "    G = nx.read_gexf(GRAPH_PATH)\n",
        "else:\n",
        "    print(\"Step 3: Building Video Graph...\")\n",
        "    G = build_video_projection_sparse(df)\n",
        "    nx.write_gexf(G, GRAPH_PATH)\n",
        "    print(f\"Saved graph to {GRAPH_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WUayC3mqSHRH",
      "metadata": {
        "id": "WUayC3mqSHRH"
      },
      "source": [
        "# 1.4 Calculate metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bZIqMupzSLPt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZIqMupzSLPt",
        "outputId": "a7a2c525-6c8a-45c4-8acb-348ba26c5256"
      },
      "outputs": [],
      "source": [
        "METRICS_PATH = PROJECT_PATH + 'data/video_metrics.csv'\n",
        "\n",
        "if os.path.exists(METRICS_PATH):\n",
        "    print(\"Checkpoint 4: Loading metrics...\")\n",
        "    df_metrics = pd.read_csv(METRICS_PATH)\n",
        "    # Rebuild partition dict for later cells\n",
        "    partition = dict(zip(df_metrics['video_id'], df_metrics['community']))\n",
        "else:\n",
        "    print(\"Step 4: Calculating Metrics...\")\n",
        "    # --- Community Detection ---\n",
        "    import community.community_louvain as community_louvain\n",
        "    partition = community_louvain.best_partition(G)\n",
        "\n",
        "    # --- Centrality ---\n",
        "    # (Use GPU code if you have it, otherwise CPU approx)\n",
        "    clustering = nx.clustering(G, weight='weight')\n",
        "    betweenness = nx.betweenness_centrality(G, k=100, weight='weight', seed=42) # Approx for speed\n",
        "\n",
        "    # Save\n",
        "    data = []\n",
        "    for n in G.nodes():\n",
        "        data.append({\n",
        "            'video_id': n,\n",
        "            'community': partition.get(n),\n",
        "            'clustering': clustering.get(n, 0),\n",
        "            'betweenness': betweenness.get(n, 0)\n",
        "        })\n",
        "    df_metrics = pd.DataFrame(data)\n",
        "    df_metrics.to_csv(METRICS_PATH, index=False)\n",
        "    print(f\"Saved metrics to {METRICS_PATH}\")\n",
        "\n",
        "print(\"\\n PIPELINE COMPLETE! You are ready for the new analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MC2ez8mAS5Pe",
      "metadata": {
        "id": "MC2ez8mAS5Pe"
      },
      "source": [
        "# Part 2: Channel-Category Network (new idea)\n",
        "That's a very clear and sophisticated insight! You've identified a common problem in network analysis called “supernode bias.”\n",
        "Implementation of a new method to circumvent “supernode bias,” a common problem in network analysis.\n",
        "To explain what this problem is, let's take an example: If “PewDiePie” has 1 million active fans who only comment on his videos, your current video graph looks like a giant ball of interconnected PewDiePie videos, which drowns out interesting connections to other channels.\n",
        "\n",
        "To solve this problem, we move from a network of videos to a network of channels by grouping users of the same channel:\n",
        "\n",
        "- **The strategy: channel-to-channel projection**\n",
        "\n",
        "  - Instead of connecting video A to video B, we will connect channel X to channel Y.\n",
        "\n",
        "  - Node: a channel (e.g., “MrBeast”).\n",
        "\n",
        "  - Edge: the number of unique users who have commented on both channel X and channel Y.\n",
        "\n",
        "  - Normalization: we must divide by the size of the channel to prevent large channels from dominating simply because they are large."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qfv1AO44U3n8",
      "metadata": {
        "id": "Qfv1AO44U3n8"
      },
      "source": [
        "Now that you have completed step 1.2 (Enrichment) above, your df contains the essential columns relating to channels and categories. We can now implement your idea: “Compare channels based on common users, distinguishing between categories.”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ymKrB8elYAYn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "4b9e67630fba492798d09efa7f54d21c",
            "5603222f22c2430f91ce536a11261d00",
            "e11a4483edbb4cce887c3b021effe525",
            "b5cbc1ab3a4748219eac2ebdca643a36",
            "e1e7b0d97f4343a19c02221ec3e1f22b",
            "55604dc3a63248248735b2356ebc278c",
            "c8615b6b895d4d8995622ff78520cf3e",
            "d4600d714f474a1fb297449b7f540188",
            "6211db8745aa42bd8bfe43153703bf87",
            "3843e37d3f1547d3833c0073285173cd",
            "cee5f8667cbf4e4d88e223cf4bc10df9"
          ]
        },
        "id": "ymKrB8elYAYn",
        "outputId": "fea62f5f-e82d-4009-f1a1-33fcd0d1bd2d"
      },
      "outputs": [],
      "source": [
        "def build_channel_category_network(df, min_shared_users=50):\n",
        "    print(\"Building Channel-Category Network (ID Mode)...\")\n",
        "\n",
        "    # 1. Create Node IDs\n",
        "    # ---------------------------------------------\n",
        "    # Format: \"ChannelID | Category\"\n",
        "    df['node_id'] = df['channel'].astype(str) + \" | \" + df['category'].astype(str)\n",
        "\n",
        "    # 2. Group Users (The heavy lifting)\n",
        "    # ---------------------------------------------\n",
        "    print(\"   Grouping users by channel...\")\n",
        "    node_audiences = df.groupby('node_id')['author_id'].apply(set).to_dict()\n",
        "\n",
        "    # Filter small nodes\n",
        "    valid_nodes = {k: v for k, v in node_audiences.items() if len(v) > min_shared_users}\n",
        "    node_list = list(valid_nodes.keys())\n",
        "\n",
        "    # 3. Build Edges\n",
        "    # ---------------------------------------------\n",
        "    edges = []\n",
        "    print(f\"   Computing overlaps for {len(node_list)} active nodes...\")\n",
        "\n",
        "    for u, v in tqdm(combinations(node_list, 2), total=len(node_list)*(len(node_list)-1)//2):\n",
        "        users_u = valid_nodes[u]\n",
        "        users_v = valid_nodes[v]\n",
        "\n",
        "        shared = len(users_u.intersection(users_v))\n",
        "\n",
        "        if shared >= min_shared_users:\n",
        "            union = len(users_u.union(users_v))\n",
        "            weight = shared / union\n",
        "            edges.append((u, v, weight, shared))\n",
        "\n",
        "    # 4. Create Graph\n",
        "    # ---------------------------------------------\n",
        "    G_cc = nx.Graph()\n",
        "    for n, users in valid_nodes.items():\n",
        "        parts = n.split(\" | \")\n",
        "        chan_id = parts[0]\n",
        "        cat = parts[1] if len(parts) > 1 else \"Unknown\"\n",
        "\n",
        "        # --- THE CHANGE ---\n",
        "        # Label: \"UC123... (Music)\"\n",
        "        # Precise, consistent, and instant to generate.\n",
        "        display_label = f\"{chan_id} ({cat})\"\n",
        "\n",
        "        G_cc.add_node(\n",
        "            n,\n",
        "            label=display_label,     # <--- What you see in plots\n",
        "            id_only=chan_id,         # <--- Raw ID\n",
        "            category=cat,            # <--- Category for coloring\n",
        "            size=len(users)\n",
        "        )\n",
        "\n",
        "    for u, v, w, s in edges:\n",
        "        G_cc.add_edge(u, v, weight=w, shared_users=s)\n",
        "\n",
        "    print(f\"Graph Built: {G_cc.number_of_nodes()} nodes.\")\n",
        "    print(f\"   Sample Label: {list(nx.get_node_attributes(G_cc, 'label').values())[0]}\")\n",
        "    return G_cc\n",
        "\n",
        "# Execute\n",
        "G_channels = build_channel_category_network(df, min_shared_users=50)\n",
        "\n",
        "# Save it to look at in Gephi!\n",
        "nx.write_gexf(G_channels, PROJECT_PATH + \"data/channel_category_network.gexf\")\n",
        "print(\"  Saved Channel Network! Download 'channel_category_network.gexf' and open in Gephi.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lSNdv76biAji",
      "metadata": {
        "id": "lSNdv76biAji"
      },
      "source": [
        "# Part 3: Comparison of the two methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PU8lQASAZ7er",
      "metadata": {
        "id": "PU8lQASAZ7er"
      },
      "source": [
        "graphs loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60T-S__FZ9g9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60T-S__FZ9g9",
        "outputId": "970028c7-4b1f-42d3-eb93-10c04abe1c4c"
      },
      "outputs": [],
      "source": [
        "# 1. Setup Path (Adjust if your path is different!)\n",
        "PROJECT_PATH = '/content/drive/My Drive/ADA_project/'\n",
        "\n",
        "print(\"Loading Graphs from disk...\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# LOAD MODEL A: Video Network (G)\n",
        "# ------------------------------------------\n",
        "video_graph_path = PROJECT_PATH + \"data/video_network.gexf\"\n",
        "\n",
        "if os.path.exists(video_graph_path):\n",
        "    print(f\"   Loading Video Graph (G) from: {video_graph_path}...\")\n",
        "    G = nx.read_gexf(video_graph_path)\n",
        "    print(f\"   Loaded G: {G.number_of_nodes()} videos, {G.number_of_edges()} edges.\")\n",
        "else:\n",
        "    print(f\"   File not found: {video_graph_path}\")\n",
        "    print(\"      -> You need to run 'Part 1: Build & Save Graph' first.\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# LOAD MODEL B: Channel Network (G_channels)\n",
        "# ------------------------------------------\n",
        "channel_graph_path = PROJECT_PATH + \"data/channel_category_network.gexf\"\n",
        "\n",
        "if os.path.exists(channel_graph_path):\n",
        "    print(f\"   Loading Channel Graph (G_channels) from: {channel_graph_path}...\")\n",
        "    G_channels = nx.read_gexf(channel_graph_path)\n",
        "    print(f\"   Loaded G_channels: {G_channels.number_of_nodes()} channels, {G_channels.number_of_edges()} edges.\")\n",
        "else:\n",
        "    print(f\"   File not found: {channel_graph_path}\")\n",
        "    print(\"      -> You need to run the 'New Idea' (Channel-Category Network) code first.\")\n",
        "\n",
        "print(\"\\n Ready for Analysis!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZVzg249UkCtk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVzg249UkCtk",
        "outputId": "fb229bd7-664e-49cd-8a15-760e63cc0e72"
      },
      "outputs": [],
      "source": [
        "def compare_structure(G_video, G_channel):\n",
        "    # 1. Modularity (Community Strength)\n",
        "    part_v = community_louvain.best_partition(G_video)\n",
        "    mod_v = community_louvain.modularity(part_v, G_video)\n",
        "\n",
        "    part_c = community_louvain.best_partition(G_channel)\n",
        "    mod_c = community_louvain.modularity(part_c, G_channel)\n",
        "\n",
        "    # 2. Density\n",
        "    den_v = nx.density(G_video)\n",
        "    den_c = nx.density(G_channel)\n",
        "\n",
        "    print(f\"Modularity (Clarity of Clusters): Video={mod_v:.3f} vs Channel={mod_c:.3f}\")\n",
        "    print(f\"Density (Connectedness):        Video={den_v:.5f} vs Channel={den_c:.5f}\")\n",
        "\n",
        "compare_structure(G, G_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ldvqMmjtlW-_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldvqMmjtlW-_",
        "outputId": "9b471003-4e01-4cc5-aaf2-c63990becb3e"
      },
      "outputs": [],
      "source": [
        "def run_comparative_analysis(G_video, G_channel):\n",
        "    print(\"==============================================\")\n",
        "    print(\"  MODEL A (Video) vs. MODEL B (Channel)  \")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "    # --- 1. STRUCTURAL COMPARISON ---\n",
        "    print(\"  STRUCTURAL HEALTH CHECK\")\n",
        "\n",
        "    den_v = nx.density(G_video)\n",
        "    den_c = nx.density(G_channel)\n",
        "\n",
        "    # Use weight='weight' for better stability\n",
        "    part_v = community_louvain.best_partition(G_video, weight='weight')\n",
        "    mod_v = community_louvain.modularity(part_v, G_video)\n",
        "\n",
        "    part_c = community_louvain.best_partition(G_channel, weight='weight')\n",
        "    mod_c = community_louvain.modularity(part_c, G_channel)\n",
        "\n",
        "    print(f\"{'Metric':<20} | {'Video Network (A)':<20} | {'Channel Network (B)':<20}\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Nodes':<20} | {G_video.number_of_nodes():<20,} | {G_channel.number_of_nodes():<20,}\")\n",
        "    print(f\"{'Edges':<20} | {G_video.number_of_edges():<20,} | {G_channel.number_of_edges():<20,}\")\n",
        "    print(f\"{'Density':<20} | {den_v:.6f}             | {den_c:.6f}\")\n",
        "    print(f\"{'Modularity':<20} | {mod_v:.3f}             | {mod_c:.3f}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # --- 2. SEMANTIC PURITY TEST ---\n",
        "    print(\"\\n SEMANTIC PURITY TEST\")\n",
        "\n",
        "    def get_purity(G, partition):\n",
        "        # Find largest community\n",
        "        df_p = pd.DataFrame.from_dict(partition, orient='index', columns=['comm_id'])\n",
        "        largest_comm_id = df_p['comm_id'].value_counts().idxmax()\n",
        "        nodes_in_largest = df_p[df_p['comm_id'] == largest_comm_id].index\n",
        "\n",
        "        # Check 'category' attribute\n",
        "        categories = []\n",
        "        for n in nodes_in_largest:\n",
        "            cat = G.nodes[n].get('category', 'Unknown')\n",
        "            categories.append(str(cat))\n",
        "\n",
        "        if not categories: return 0, \"N/A\"\n",
        "\n",
        "        # Most common category\n",
        "        top_cat, count = Counter(categories).most_common(1)[0]\n",
        "        return (count / len(categories)) * 100, top_cat\n",
        "\n",
        "    purity_c, cat_c = get_purity(G_channel, part_c)\n",
        "\n",
        "    print(f\"   -> In Model B (Channel), the largest community is dominated by '{cat_c}'.\")\n",
        "    print(f\"   -> Purity Score: {purity_c:.1f}% of channels there are '{cat_c}'.\")\n",
        "\n",
        "    # --- 3. UPDATED RECOMMENDER SIMULATION ---\n",
        "    print(\"\\n  RECOMMENDER SIMULATION (Live Demo)\")\n",
        "\n",
        "    # A. AUTOMATIC MODE: Pick the biggest node (The Hub)\n",
        "    # Since we don't know if 'MrBeast' is in your data, we pick the most connected node.\n",
        "    sorted_nodes = sorted(G_channel.degree, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if sorted_nodes:\n",
        "        hub_node = sorted_nodes[0][0] # The ID of the biggest channel\n",
        "\n",
        "        # Try to make it readable (ID | Category)\n",
        "        print(f\"   Input Channel: '{hub_node}'\")\n",
        "\n",
        "        # Get top 3 neighbors by weight\n",
        "        neighbors = sorted(G_channel[hub_node].items(), key=lambda x: x[1]['weight'], reverse=True)[:3]\n",
        "\n",
        "        print(f\"   Recommendations (based on shared audience):\")\n",
        "        for neighbor, attr in neighbors:\n",
        "            print(f\"      - {neighbor} (Strength: {attr['weight']:.2f})\")\n",
        "    else:\n",
        "        print(\"   (Graph is empty, cannot run simulation.)\")\n",
        "\n",
        "# --- RUN IT ---\n",
        "if 'G' in locals() and 'G_channels' in locals():\n",
        "    run_comparative_analysis(G, G_channels)\n",
        "else:\n",
        "    print(\"Error: Load G and G_channels first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iUWiD9Q6mP8T",
      "metadata": {
        "id": "iUWiD9Q6mP8T"
      },
      "source": [
        "### General Graphs Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pLxBsoWGmSsX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pLxBsoWGmSsX",
        "outputId": "45bd36b0-fd6d-4a49-8035-9ddc95085339"
      },
      "outputs": [],
      "source": [
        "def visualize_network_skeleton(G, title, top_n=200, show_labels=False):\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    # 1. FILTER: Keep only the most connected nodes (The \"Skeleton\")\n",
        "    # Sort nodes by degree\n",
        "    sorted_nodes = sorted(G.degree, key=lambda x: x[1], reverse=True)\n",
        "    top_nodes = [n for n, d in sorted_nodes[:top_n]]\n",
        "\n",
        "    # Create the subgraph\n",
        "    H = G.subgraph(top_nodes)\n",
        "\n",
        "    # 2. DETECT COMMUNITIES (for coloring)\n",
        "    # Recalculate partition just for this subgraph view\n",
        "    partition = community_louvain.best_partition(H)\n",
        "\n",
        "    # 3. LAYOUT (Force-directed)\n",
        "    # k controls the spacing between nodes (higher = more spread out)\n",
        "    pos = nx.spring_layout(H, k=0.15, seed=42)\n",
        "\n",
        "    # 4. DRAW\n",
        "    # Nodes\n",
        "    nx.draw_networkx_nodes(H, pos,\n",
        "                           node_size=[v * 5 for v in dict(H.degree).values()], # Size by degree\n",
        "                           cmap=plt.cm.tab20,\n",
        "                           node_color=list(partition.values()),\n",
        "                           alpha=0.9)\n",
        "\n",
        "    # Edges (Make them faint)\n",
        "    nx.draw_networkx_edges(H, pos, alpha=0.1, edge_color='gray')\n",
        "\n",
        "    # Labels (Only if requested)\n",
        "    if show_labels:\n",
        "        # Only label the very top 20 to avoid clutter\n",
        "        top_20 = top_nodes[:20]\n",
        "        labels = {n: n.split(\" | \")[0] for n in top_20} # Clean labels for Channels\n",
        "        nx.draw_networkx_labels(H, pos, labels, font_size=10, font_weight='bold')\n",
        "\n",
        "    plt.title(f\"{title}\\n(Top {top_n} Nodes)\", fontsize=15)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# --- RUN THE VISUALIZATION ---\n",
        "\n",
        "print(\"Drawing Model A: Video Network (The 'Hairball')...\")\n",
        "# Video graph is usually messy, no labels needed\n",
        "visualize_network_skeleton(G, \"Model A: Video Connectivity\", top_n=50, show_labels=False)\n",
        "\n",
        "print(\"Drawing Model B: Channel-Category Network (The 'Map')...\")\n",
        "# Channel graph is cleaner, we WANT to see the names!\n",
        "if 'G_channels' in locals():\n",
        "    visualize_network_skeleton(G_channels, \"Model B: Channel Ecosystem\", top_n=50, show_labels=False)\n",
        "else:\n",
        "    print(\"G_channels not found. Run the 'New Idea' code block first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0A6wwfBh0iOr",
      "metadata": {
        "id": "0A6wwfBh0iOr"
      },
      "source": [
        "- Same Color = Same Audience Tribe.\n",
        "- Different Color = Different Audience segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GXgerADNm9XZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "GXgerADNm9XZ",
        "outputId": "13c8b9a5-b8c0-4742-a56c-2f2c60bdf286"
      },
      "outputs": [],
      "source": [
        "def plot_l_curve_comparison(G_video, G_channel):\n",
        "    print(\"Generating L-Curve (Degree Distribution) Comparison...\")\n",
        "\n",
        "    # 1. Get Degrees (Connectivity)\n",
        "    # -----------------------------\n",
        "    # List of degrees for every node (e.g., [1, 5, 200, 2, ...])\n",
        "    degrees_v = [d for n, d in G_video.degree()]\n",
        "    degrees_c = [d for n, d in G_channel.degree()]\n",
        "\n",
        "    # 2. Setup the Plot (2 Panels)\n",
        "    # -----------------------------\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # --- PANEL 1: The \"L-Curve\" (Linear Scale) ---\n",
        "    # This shows the \"Long Tail\" explicitly\n",
        "\n",
        "    # Histogram for Video Network\n",
        "    axes[0].hist(degrees_v, bins=50, color='#1f77b4', alpha=0.6, label='Video Network', density=True)\n",
        "    # Histogram for Channel Network\n",
        "    axes[0].hist(degrees_c, bins=50, color='#ff7f0e', alpha=0.6, label='Channel Network', density=True)\n",
        "\n",
        "    axes[0].set_title(\"The 'L-Curve' (Linear Scale)\\n(Extreme Inequality)\", fontsize=14)\n",
        "    axes[0].set_xlabel(\"Degree (Number of Connections)\")\n",
        "    axes[0].set_ylabel(\"Frequency (Probability)\")\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "\n",
        "    # --- PANEL 2: The \"Power Law\" (Log-Log Scale) ---\n",
        "    # This checks if it follows the \"Rich-get-Richer\" rule\n",
        "\n",
        "    # Helper to calculate PDF (Probability Density Function)\n",
        "    def get_pdf(data):\n",
        "        values, counts = np.unique(data, return_counts=True)\n",
        "        probs = counts / len(data)\n",
        "        return values, probs\n",
        "\n",
        "    x_v, y_v = get_pdf(degrees_v)\n",
        "    x_c, y_c = get_pdf(degrees_c)\n",
        "\n",
        "    axes[1].loglog(x_v, y_v, 'o', color='#1f77b4', alpha=0.5, markersize=3, label='Video Network')\n",
        "    axes[1].loglog(x_c, y_c, 's', color='#ff7f0e', alpha=0.5, markersize=4, label='Channel Network')\n",
        "\n",
        "    axes[1].set_title(\"The 'Power Law' (Log-Log Scale)\\n(Straight Line = Scale-Free)\", fontsize=14)\n",
        "    axes[1].set_xlabel(\"Degree (Log Scale)\")\n",
        "    axes[1].set_ylabel(\"Frequency (Log Scale)\")\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3, which=\"both\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- STATS ---\n",
        "    print(\"\\n NETWORK INEQUALITY STATS:\")\n",
        "    print(f\"   Max Degree (Video):   {max(degrees_v):,} connections (The Viral Hit)\")\n",
        "    print(f\"   Max Degree (Channel): {max(degrees_c):,} connections (The Super-Influencer)\")\n",
        "    print(f\"   Avg Degree (Video):   {np.mean(degrees_v):.1f}\")\n",
        "    print(f\"   Avg Degree (Channel): {np.mean(degrees_c):.1f}\")\n",
        "\n",
        "# --- EXECUTE ---\n",
        "if 'G' in locals() and 'G_channels' in locals():\n",
        "    plot_l_curve_comparison(G, G_channels)\n",
        "else:\n",
        "    print(\"Please run the graph building steps first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WiOk9aXJj9u8",
      "metadata": {
        "id": "WiOk9aXJj9u8"
      },
      "source": [
        "Nodes are labeled by their most representative video title because channel names were anonymized/missing in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9K8hCQWDcyg7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9K8hCQWDcyg7",
        "outputId": "7d8dd333-147d-4642-8d6d-c9917f3ac01f"
      },
      "outputs": [],
      "source": [
        "# --- 1. DEFINE THE REPORT FUNCTION ---\n",
        "def generate_network_report(G, partition, title=\"Network Analysis\"):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"REPORT: {title}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # A. GLOBAL STATS\n",
        "    n_nodes = G.number_of_nodes()\n",
        "    n_edges = G.number_of_edges()\n",
        "    density = nx.density(G)\n",
        "    components = list(nx.connected_components(G))\n",
        "    giant_component = len(components[0]) if components else 0\n",
        "\n",
        "    print(f\"• Nodes:                {n_nodes:,}\")\n",
        "    print(f\"• Edges:                {n_edges:,}\")\n",
        "    print(f\"• Density:              {density:.6f}\")\n",
        "    print(f\"• Connected Components: {len(components)}\")\n",
        "    print(f\"• Giant Component:      {giant_component:,} ({giant_component/n_nodes:.1%})\")\n",
        "\n",
        "    # B. PREPARE DATA\n",
        "    # We check if metrics exist in the graph; if not, we use 0\n",
        "    data = []\n",
        "    for node in G.nodes():\n",
        "        # Handle Channel vs Video ID labeling\n",
        "        label = G.nodes[node].get('label', node)\n",
        "\n",
        "        data.append({\n",
        "            'node_id': node,\n",
        "            'label': label,\n",
        "            'community': partition.get(node, -1),\n",
        "            'degree': G.degree(node),\n",
        "            'clustering': G.nodes[node].get('clustering_coefficient', 0),\n",
        "            'betweenness': G.nodes[node].get('betweenness', 0)\n",
        "        })\n",
        "    df_net = pd.DataFrame(data)\n",
        "\n",
        "    n_communities = df_net['community'].nunique()\n",
        "    print(f\"• Communities Detected: {n_communities}\")\n",
        "\n",
        "    # C. PLOT 1: Community Sizes\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    comm_counts = df_net['community'].value_counts().head(20)\n",
        "    sns.barplot(x=comm_counts.index, y=comm_counts.values, palette=\"viridis\", hue=comm_counts.index, legend=False)\n",
        "    plt.title(f\"{title}: Top 20 Communities\")\n",
        "    plt.xlabel(\"Community ID\")\n",
        "    plt.ylabel(\"Size\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    # D. LANDMARKS\n",
        "    print(f\"\\n TOP LANDMARKS ({title})\")\n",
        "\n",
        "    # Bridges\n",
        "    if df_net['betweenness'].max() > 0:\n",
        "        print(\"\\nTOP 5 BRIDGES (Connectors):\")\n",
        "        display(df_net.nlargest(5, 'betweenness')[['label', 'community', 'betweenness', 'degree']])\n",
        "    else:\n",
        "        print(\"\\n(Betweenness not calculated, skipping Bridge table)\")\n",
        "\n",
        "    # Cores\n",
        "    if df_net['clustering'].max() > 0:\n",
        "        print(\"\\nTOP 5 CORES (Echo Chambers):\")\n",
        "        display(df_net[df_net['degree'] > 5].nlargest(5, 'clustering')[['label', 'community', 'clustering', 'degree']])\n",
        "\n",
        "    # E. PLOT 2: Hypothesis Check\n",
        "    if df_net['betweenness'].max() > 0:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.scatterplot(data=df_net, x='clustering', y='betweenness', alpha=0.5, size='degree', sizes=(10, 200))\n",
        "        plt.title(f\"{title}: Bridge vs. Core\")\n",
        "        plt.xlabel(\"Clustering (Core Score)\")\n",
        "        plt.ylabel(\"Betweenness (Bridge Score)\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "    return df_net\n",
        "\n",
        "# --- 2. RUN ON VIDEO NETWORK (G) ---\n",
        "if 'G' in locals():\n",
        "    # Attempt to retrieve partition from graph, or recalculate if missing\n",
        "    # (Assuming you saved 'community' attribute in G)\n",
        "    part_v = nx.get_node_attributes(G, 'community')\n",
        "    if not part_v:\n",
        "        print(\"Partition not found in G, running simple detection...\")\n",
        "        part_v = community_louvain.best_partition(G)\n",
        "\n",
        "    df_video_stats = generate_network_report(G, part_v, title=\"Video Network (Model A)\")\n",
        "else:\n",
        "    print(\"G (Video Graph) is not loaded.\")\n",
        "\n",
        "# --- 3. RUN ON CHANNEL NETWORK (G_channels) ---\n",
        "if 'G_channels' in locals():\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Since G_channels is small, let's Calculate Metrics on the fly to ensure plots work!\n",
        "    print(\"Calculating metrics for Channel Network (Fast)...\")\n",
        "    part_c = community_louvain.best_partition(G_channels)\n",
        "    bc_c = nx.betweenness_centrality(G_channels, weight='weight')\n",
        "    cc_c = nx.clustering(G_channels, weight='weight')\n",
        "\n",
        "    # Store in graph so the function finds them\n",
        "    nx.set_node_attributes(G_channels, bc_c, 'betweenness')\n",
        "    nx.set_node_attributes(G_channels, cc_c, 'clustering_coefficient')\n",
        "\n",
        "    df_channel_stats = generate_network_report(G_channels, part_c, title=\"Channel Network (Model B)\")\n",
        "else:\n",
        "    print(\"G_channels is not loaded. Run the 'New Idea' cells first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pbdE5tz-qv-t",
      "metadata": {
        "id": "pbdE5tz-qv-t"
      },
      "source": [
        "Generate clickable links:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j105qo68qzI4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j105qo68qzI4",
        "outputId": "6e7cc0e8-8e25-4bc3-d158-716aebdbd0c4"
      },
      "outputs": [],
      "source": [
        "def list_top_channels_with_links(G_channel, top_k=10):\n",
        "    print(f\"Top {top_k} Channels (Click to Verify):\\n\")\n",
        "\n",
        "    # Sort by degree (connectivity) or size\n",
        "    sorted_nodes = sorted(G_channel.nodes(data=True), key=lambda x: x[1]['size'], reverse=True)[:top_k]\n",
        "\n",
        "    for node_id, attr in sorted_nodes:\n",
        "        # Extract the pure Channel ID (remove the \"| Category\" part if present)\n",
        "        # If you used the \"ID Mode\" code, the ID is stored in 'id_only' or is the first part of the string\n",
        "        c_id = attr.get('id_only', node_id.split(\" | \")[0])\n",
        "        category = attr.get('category', 'Unknown')\n",
        "\n",
        "        url = f\"https://www.youtube.com/channel/{c_id}\"\n",
        "\n",
        "        print(f\"{attr.get('label', node_id)}\")\n",
        "        print(f\"   Category: {category}\")\n",
        "        print(f\"   Link: {url}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# --- RUN IT ---\n",
        "if 'G_channels' in locals():\n",
        "    list_top_channels_with_links(G_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NYRrjCC5wijb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYRrjCC5wijb",
        "outputId": "d53298e7-e5c4-41f6-8e8a-c4507fdcdee6"
      },
      "outputs": [],
      "source": [
        "def benchmark_methods(df, n_rows=100_000):\n",
        "    print(f\"STARTING BENCHMARK on first {n_rows:,} rows...\\n\")\n",
        "\n",
        "    # 1. PREPARE DATA SAMPLE\n",
        "    # ----------------------\n",
        "    df_sample = df.head(n_rows)[['author_id', 'video_id']].drop_duplicates().copy()\n",
        "    print(f\"   Sample size: {len(df_sample):,} rows\")\n",
        "    print(f\"   Unique Users: {df_sample['author_id'].nunique():,}\")\n",
        "    print(f\"   Unique Videos: {df_sample['video_id'].nunique():,}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # =========================================================\n",
        "    # METHOD 1: YOUR INITIAL IMPLEMENTATION (Iterative Loops)\n",
        "    # =========================================================\n",
        "    print(\"METHOD 1: Iterative Loop (Your Code)...\")\n",
        "    start_1 = time.time()\n",
        "\n",
        "    # A. Build User->Video Mapping\n",
        "    user_to_videos = df_sample.groupby('author_id')['video_id'].apply(list).to_dict()\n",
        "\n",
        "    # B. Compute Edges (The Bottleneck)\n",
        "    edge_overlaps = defaultdict(int)\n",
        "    for user, vids in user_to_videos.items():\n",
        "        if len(vids) >= 2:\n",
        "            # This 'combinations' step has complexity O(N^2) per user\n",
        "            for v1, v2 in combinations(sorted(vids), 2):\n",
        "                edge_overlaps[(v1, v2)] += 1\n",
        "\n",
        "    # C. Build DataFrame (to match your output format)\n",
        "    edge_list_1 = [{'source': k[0], 'target': k[1], 'weight': v} for k, v in edge_overlaps.items()]\n",
        "    df_edges_1 = pd.DataFrame(edge_list_1)\n",
        "\n",
        "    time_1 = time.time() - start_1\n",
        "    print(f\"   Time: {time_1:.4f} seconds\")\n",
        "    print(f\"   edges found: {len(df_edges_1):,}\")\n",
        "\n",
        "    # =========================================================\n",
        "    # METHOD 2: MATRIX MULTIPLICATION (Linear Algebra)\n",
        "    # =========================================================\n",
        "    print(\"\\n METHOD 2: Sparse Matrix Multiplication...\")\n",
        "    start_2 = time.time()\n",
        "\n",
        "    # A. Map IDs to Integers\n",
        "    users = df_sample['author_id'].unique()\n",
        "    videos = df_sample['video_id'].unique()\n",
        "\n",
        "    user_map = {u: i for i, u in enumerate(users)}\n",
        "    video_map = {v: i for i, v in enumerate(videos)}\n",
        "\n",
        "    # B. Create Sparse Matrix (Users x Videos)\n",
        "    row_ind = df_sample['author_id'].map(user_map).values\n",
        "    col_ind = df_sample['video_id'].map(video_map).values\n",
        "    data = np.ones(len(df_sample), dtype=int)\n",
        "\n",
        "    # Shape: (Rows=Users, Cols=Videos)\n",
        "    M = sp.csr_matrix((data, (row_ind, col_ind)), shape=(len(users), len(videos)))\n",
        "\n",
        "    # C. The Magic Trick: M.T * M = (Video x User) * (User x Video) = Video x Video\n",
        "    # This computes ALL shared counts in one step\n",
        "    V = M.T @ M\n",
        "\n",
        "    # D. Extract Edges\n",
        "    # Remove self-loops (diagonal)\n",
        "    V.setdiag(0)\n",
        "    V.eliminate_zeros()\n",
        "\n",
        "    # Get upper triangle only (to avoid duplicates A-B and B-A)\n",
        "    V_tri = sp.triu(V)\n",
        "\n",
        "    # Convert to standard format\n",
        "    rows, cols = V_tri.nonzero()\n",
        "    weights = V_tri.data\n",
        "\n",
        "    # (Optional: map back to string IDs - included for fair comparison)\n",
        "    # reverse_video_map = {i: v for v, i in video_map.items()}\n",
        "    # source_ids = [reverse_video_map[r] for r in rows]\n",
        "    # target_ids = [reverse_video_map[c] for c in cols]\n",
        "\n",
        "    time_2 = time.time() - start_2\n",
        "    print(f\"   Time: {time_2:.4f} seconds\")\n",
        "    print(f\"   edges found: {len(weights):,}\")\n",
        "\n",
        "    # =========================================================\n",
        "    # RESULTS\n",
        "    # =========================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL VERDICT\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if time_2 > 0:\n",
        "        speedup = time_1 / time_2\n",
        "        print(f\"Matrix Method is {speedup:.1f}x FASTER\")\n",
        "    else:\n",
        "        print(\"Matrix Method was too fast to measure.\")\n",
        "\n",
        "    print(f\"\\nWhy? Method 1 complexity is O(Users * VideosPerUser^2)\")\n",
        "    print(f\"If a user watched 50 videos:\")\n",
        "    print(f\"   - Method 1 runs a loop 1,225 times (50*49/2)\")\n",
        "    print(f\"   - Method 2 does this in highly optimized C-code.\")\n",
        "\n",
        "# --- EXECUTE ---\n",
        "if 'df' in locals():\n",
        "    # Ensure columns match your dataset (rename if needed)\n",
        "    # df.rename(columns={'author': 'author_id'}, inplace=True)\n",
        "    benchmark_methods(df, n_rows=100_000)\n",
        "else:\n",
        "    print(\"Please load your 'df' dataframe first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tuaWbNwv7Cyp",
      "metadata": {
        "id": "tuaWbNwv7Cyp"
      },
      "source": [
        "## Data analysis on the graph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AESX2swF7L1m",
      "metadata": {
        "id": "AESX2swF7L1m"
      },
      "source": [
        "### 1. The Macro View: Which \"Continents\" (Categories) interact? (e.g., \"Do Gamers watch News?\")\n",
        "E.G. The Story: \"First, we zoomed out to see how the major interest groups interact. We discovered that the 'Gaming' community is highly isolated, while 'Music' acts as a universal language connecting everyone.\"\n",
        "\n",
        "A Heatmap of cross-category traffic:\n",
        "- **Interpretation:**\n",
        "  - Bright Squares off-diagonal: Strong connections (e.g., if Gaming intersects Tech, that square will be bright).\n",
        "  - Dark Zones: Segregation (e.g., Politics might not touch Gaming)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coX0Zc537Vlw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "coX0Zc537Vlw",
        "outputId": "eb1a48e4-b495-4724-b027-4d7ec5c1a93d"
      },
      "outputs": [],
      "source": [
        "def analyze_category_flow(G_channel):\n",
        "    print(\"The Category Interaction Matrix:\")\n",
        "\n",
        "    # 1. Aggregation: Sum weights between categories\n",
        "    category_interaction = {}\n",
        "\n",
        "    for u, v, data in G_channel.edges(data=True):\n",
        "        cat_u = G_channel.nodes[u].get('category', 'Unknown')\n",
        "        cat_v = G_channel.nodes[v].get('category', 'Unknown')\n",
        "\n",
        "        # Sort to ensure A-B is same as B-A\n",
        "        pair = tuple(sorted([cat_u, cat_v]))\n",
        "        weight = data.get('weight', 0)\n",
        "\n",
        "        if pair not in category_interaction:\n",
        "            category_interaction[pair] = 0\n",
        "        category_interaction[pair] += weight\n",
        "\n",
        "    # 2. Convert to DataFrame for Heatmap\n",
        "    # We list all unique categories\n",
        "    categories = sorted(list(set([c for pair in category_interaction.keys() for c in pair])))\n",
        "    matrix = pd.DataFrame(0.0, index=categories, columns=categories)\n",
        "\n",
        "    for (c1, c2), w in category_interaction.items():\n",
        "        matrix.loc[c1, c2] = w\n",
        "        matrix.loc[c2, c1] = w # Symmetric\n",
        "\n",
        "    # Normalize by diagonal (Self-loop) to see relative strength?\n",
        "    # Or just log scale. Let's use Log scale for visibility.\n",
        "    import numpy as np\n",
        "    matrix_log = np.log1p(matrix)\n",
        "\n",
        "    # 3. Plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(matrix_log, annot=False, cmap=\"magma\", linewidths=.5)\n",
        "    plt.title(\"Heatmap of Audience Overlap between Categories\\n(Brighter = Stronger Connection)\")\n",
        "    plt.show()\n",
        "\n",
        "    return matrix\n",
        "\n",
        "# --- RUN IT ---\n",
        "if 'G_channels' in locals():\n",
        "    cat_matrix = analyze_category_flow(G_channels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gdXLTnBGAoki",
      "metadata": {
        "id": "gdXLTnBGAoki"
      },
      "source": [
        "Some considerations about the heatmap:\n",
        "- The diagonal represents Intra-Category Connectivity (e.g., How much do Gamers watch other Gaming channels?):\n",
        "\n",
        "  - Bright Diagonal (High Value): This indicates an \"Echo Chamber\" or a very tight community. (loyal = bright)\n",
        "  - Dim/Dark Diagonal (Low Value): This indicates a \"Loose Category\". (dark = more explorer, not loyal)\n",
        "\n",
        "\n",
        "- Different Colors: The color scale is also affected by Size. If \"Music\" has 10,000 channels and \"Pets\" has 100, \"Music\" will naturally have a brighter sum."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5XR1t-3CEqY",
      "metadata": {
        "id": "b5XR1t-3CEqY"
      },
      "source": [
        "### 2. Category Backbone Network\n",
        "Instead of plotting 1,000 channels, we can compress the entire graph into just ~15 nodes (one per Category). We draw a line only if there is significant audience overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YFMhRhdzCO3Z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YFMhRhdzCO3Z",
        "outputId": "97f74f91-5d14-4975-96ca-26eb567e8e48"
      },
      "outputs": [],
      "source": [
        "def plot_category_backbone(G_channel, threshold_percentile=70):\n",
        "    print(\"Generating Chapter 1.5: The 'Continent Map' (Category Backbone)...\")\n",
        "\n",
        "    # 1. BUILD THE CATEGORY GRAPH\n",
        "    # Create a new graph where Nodes = Categories\n",
        "    G_cat = nx.Graph()\n",
        "\n",
        "    # Tally up weights between categories\n",
        "    edge_weights = {}\n",
        "    node_sizes = {}\n",
        "\n",
        "    for u, v, data in G_channel.edges(data=True):\n",
        "        cat_u = G_channel.nodes[u].get('category', 'Unknown')\n",
        "        cat_v = G_channel.nodes[v].get('category', 'Unknown')\n",
        "\n",
        "        # Count node sizes (how many channels per category)\n",
        "        node_sizes[cat_u] = node_sizes.get(cat_u, 0) + 1\n",
        "        node_sizes[cat_v] = node_sizes.get(cat_v, 0) + 1\n",
        "\n",
        "        # Skip self-loops for the map (we want to see connections BETWEEN types)\n",
        "        if cat_u != cat_v:\n",
        "            pair = tuple(sorted([cat_u, cat_v]))\n",
        "            w = data.get('weight', 0)\n",
        "            edge_weights[pair] = edge_weights.get(pair, 0) + w\n",
        "\n",
        "    # 2. ADD TO GRAPH\n",
        "    # Normalize weights so the plot isn't messy\n",
        "    max_w = max(edge_weights.values()) if edge_weights else 1\n",
        "\n",
        "    # Filter: Only keep the STRONGEST links (e.g., top 30%) to make it \"Explicit\"\n",
        "    import numpy as np\n",
        "    cutoff = np.percentile(list(edge_weights.values()), threshold_percentile)\n",
        "\n",
        "    for (c1, c2), w in edge_weights.items():\n",
        "        if w > cutoff:\n",
        "            G_cat.add_edge(c1, c2, weight=w, penwidth=(w/max_w)*5)\n",
        "\n",
        "    # Add nodes (even if isolated)\n",
        "    for cat in node_sizes:\n",
        "        if cat in G_cat.nodes(): # Only if connected or force add\n",
        "            G_cat.nodes[cat]['size'] = node_sizes[cat]\n",
        "\n",
        "    # 3. VISUALIZE\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    pos = nx.spring_layout(G_cat, k=2.0, seed=42) # k=2 pushes them apart\n",
        "\n",
        "    # Draw Nodes\n",
        "    sizes = [node_sizes.get(n, 10)*5 for n in G_cat.nodes()]\n",
        "    nx.draw_networkx_nodes(G_cat, pos, node_size=sizes, node_color=\"#66c2a5\", alpha=0.9)\n",
        "\n",
        "    # Draw Edges (Thicker = Stronger connection)\n",
        "    weights = [G_cat[u][v]['penwidth'] for u, v in G_cat.edges()]\n",
        "    nx.draw_networkx_edges(G_cat, pos, width=weights, edge_color=\"gray\", alpha=0.6)\n",
        "\n",
        "    # Draw Labels\n",
        "    nx.draw_networkx_labels(G_cat, pos, font_size=10, font_weight=\"bold\",\n",
        "                            bbox=dict(facecolor=\"white\", edgecolor='none', alpha=0.7))\n",
        "\n",
        "    plt.title(\"The Youtube Continent Map\\n(Only Strongest Links Shown)\", fontsize=15)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# --- RUN IT ---\n",
        "if 'G_channels' in locals():\n",
        "    plot_category_backbone(G_channels, threshold_percentile=90)\n",
        "    # Try changing threshold (50, 80, 90) to see more/less lines!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zdoqRdns8BTM",
      "metadata": {
        "id": "zdoqRdns8BTM"
      },
      "source": [
        "### 3. Bridge Analysis\n",
        "E.G. The Story: \"We then looked for the 'Diplomats'—the channels that sit on the borders. While 'PewDiePie' is the King of Gaming, we found that [Channel X] is actually the critical bridge connecting Gamers to the Science community.\"\n",
        "\n",
        "A list of channels with high Betweenness Centrality that connect different categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ShfbVyCM8Yhv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShfbVyCM8Yhv",
        "outputId": "25d80ac0-a73b-48b1-f0d6-7edd099a5431"
      },
      "outputs": [],
      "source": [
        "def find_the_diplomats(G_channel, top_k=5):\n",
        "    print(f\"The Bridges between Categories...\")\n",
        "\n",
        "    # Calculate Betweenness (How much traffic flows THROUGH this node)\n",
        "    # limit to k items to speed up approximation if graph is huge, or run full if < 2000 nodes\n",
        "    betweenness = nx.betweenness_centrality(G_channel, weight='weight')\n",
        "\n",
        "    # Sort\n",
        "    sorted_nodes = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(f\"\\n Top {top_k} Channels that act as Bridges:\")\n",
        "    print(f\"{'Channel Name':<40} | {'Category':<15} | {'Bridge Score':<10} | {'Top Neighbor Types'}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for node_id, score in sorted_nodes[:top_k]:\n",
        "        # Get Attributes\n",
        "        label = G_channel.nodes[node_id].get('label', node_id)\n",
        "        my_cat = G_channel.nodes[node_id].get('category', 'Unknown')\n",
        "\n",
        "        # Analyze Neighbors: Who are they connecting?\n",
        "        neighbors = G_channel[node_id]\n",
        "        neighbor_cats = [G_channel.nodes[n].get('category') for n in neighbors]\n",
        "\n",
        "        # Find the most common OUTSIDE category\n",
        "        from collections import Counter\n",
        "        counts = Counter([c for c in neighbor_cats if c != my_cat])\n",
        "\n",
        "        if counts:\n",
        "            primary_bridge = f\"Links to {counts.most_common(1)[0][0]}\"\n",
        "        else:\n",
        "            primary_bridge = \"Internal Hub (Echo Chamber)\"\n",
        "\n",
        "        print(f\"{label[:38]:<40} | {my_cat[:13]:<15} | {score:.4f}     | {primary_bridge}\")\n",
        "\n",
        "# --- RUN IT ---\n",
        "if 'G_channels' in locals():\n",
        "    find_the_diplomats(G_channels, top_k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xjKcbB9s8w3R",
      "metadata": {
        "id": "xjKcbB9s8w3R"
      },
      "source": [
        "### 4. Recommendation simulation\n",
        "E.G. The Story: \"Finally, we applied this logic to a user. Starting from a 'Music' preference, our algorithm uses the 'Diplomat' nodes to guide them toward 'Entertainment', broadening their horizon without losing relevance.\"\n",
        "\n",
        "A step-by-step path trace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UkJigykN88m1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkJigykN88m1",
        "outputId": "89a43852-6f53-40f7-f322-59dce62dbb08"
      },
      "outputs": [],
      "source": [
        "def simulate_user_journey(G_channel, start_node_search_term):\n",
        "    print(\"\\n The Recommendation Journey:\")\n",
        "\n",
        "    # 1. Find a start node based on search string (e.g., \"Music\")\n",
        "    candidates = [n for n in G_channel.nodes() if start_node_search_term.lower() in str(G_channel.nodes[n].get('label', '')).lower()]\n",
        "\n",
        "    if not candidates:\n",
        "        print(f\"No channel found matching '{start_node_search_term}'\")\n",
        "        return\n",
        "\n",
        "    start_node = candidates[0]\n",
        "    start_label = G_channel.nodes[start_node].get('label')\n",
        "    start_cat = G_channel.nodes[start_node].get('category')\n",
        "\n",
        "    print(f\"START POINT: User is watching '{start_label}' ({start_cat})\")\n",
        "\n",
        "    # 2. Step 1: Immediate Neighbors (The \"Safe\" Choice)\n",
        "    neighbors = sorted(G_channel[start_node].items(), key=lambda x: x[1]['weight'], reverse=True)\n",
        "    if not neighbors: return\n",
        "\n",
        "    safe_choice = neighbors[0][0]\n",
        "    safe_label = G_channel.nodes[safe_choice].get('label')\n",
        "\n",
        "    print(f\"   \")\n",
        "    print(f\"   SAFE Recommendation: '{safe_label}' (Same Community)\")\n",
        "\n",
        "    # 3. Step 2: The \"Diplomat\" Jump (The \"Novel\" Choice)\n",
        "    # Look for a neighbor that is in a DIFFERENT category but high weight\n",
        "    novel_choice = None\n",
        "    for n, data in neighbors:\n",
        "        cat = G_channel.nodes[n].get('category')\n",
        "        if cat != start_cat:\n",
        "            novel_choice = n\n",
        "            break\n",
        "\n",
        "    if novel_choice:\n",
        "        novel_label = G_channel.nodes[novel_choice].get('label')\n",
        "        novel_cat = G_channel.nodes[novel_choice].get('category')\n",
        "        print(f\"   \")\n",
        "        print(f\"   DISCOVERY Recommendation: '{novel_label}'\")\n",
        "        print(f\"      (Why? It connects {start_cat} to {novel_cat})\")\n",
        "    else:\n",
        "        print(\"   (No cross-category bridge found from here)\")\n",
        "\n",
        "# --- RUN IT ---\n",
        "# Try searching for a word you know exists in your labels, e.g., \"Gaming\" or \"Music\" or a specific name\n",
        "if 'G_channels' in locals():\n",
        "    # Replace 'Music' with a keyword likely to be in your new readable labels!\n",
        "    simulate_user_journey(G_channels, \"Music\")\n",
        "    simulate_user_journey(G_channels, \"Gaming\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ada",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3843e37d3f1547d3833c0073285173cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b9e67630fba492798d09efa7f54d21c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5603222f22c2430f91ce536a11261d00",
              "IPY_MODEL_e11a4483edbb4cce887c3b021effe525",
              "IPY_MODEL_b5cbc1ab3a4748219eac2ebdca643a36"
            ],
            "layout": "IPY_MODEL_e1e7b0d97f4343a19c02221ec3e1f22b"
          }
        },
        "55604dc3a63248248735b2356ebc278c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5603222f22c2430f91ce536a11261d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55604dc3a63248248735b2356ebc278c",
            "placeholder": "​",
            "style": "IPY_MODEL_c8615b6b895d4d8995622ff78520cf3e",
            "value": "100%"
          }
        },
        "6211db8745aa42bd8bfe43153703bf87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5cbc1ab3a4748219eac2ebdca643a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3843e37d3f1547d3833c0073285173cd",
            "placeholder": "​",
            "style": "IPY_MODEL_cee5f8667cbf4e4d88e223cf4bc10df9",
            "value": " 676866/676866 [00:05&lt;00:00, 191539.51it/s]"
          }
        },
        "c8615b6b895d4d8995622ff78520cf3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cee5f8667cbf4e4d88e223cf4bc10df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4600d714f474a1fb297449b7f540188": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e11a4483edbb4cce887c3b021effe525": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4600d714f474a1fb297449b7f540188",
            "max": 676866,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6211db8745aa42bd8bfe43153703bf87",
            "value": 676866
          }
        },
        "e1e7b0d97f4343a19c02221ec3e1f22b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
